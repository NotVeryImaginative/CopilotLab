{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T00:13:58.084384Z",
     "start_time": "2025-12-03T00:13:57.192138Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f960d7d-b262-4dab-b6bc-cc9030e7869a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .config(\"spark.default.parallelism\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da75c0cb-603c-4233-a9ae-7fb9c435166d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark session with limited parallelism\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesTransactionSample\") \\\n",
    "    .config(\"spark.default.parallelism\", 4) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"transaction_date\", TimestampType(), False),\n",
    "    StructField(\"payment_mode\", StringType(), False),\n",
    "    StructField(\"store_location\", StringType(), False),\n",
    "    StructField(\"discount_applied\", BooleanType(), False),\n",
    "    StructField(\"discount_amount\", DoubleType(), False),\n",
    "    StructField(\"loyalty_points_earned\", IntegerType(), False),\n",
    "    StructField(\"is_return\", BooleanType(), False),\n",
    "    StructField(\"sales_rep\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"T001\", \"C001\", \"Alice Smith\", \"P001\", \"Laptop\", \"Electronics\", 1200.0, 1, datetime(2024, 6, 1, 10, 30), \"Credit Card\", \"New York\", True, 100.0, 120, False, \"SR01\"),\n",
    "    (\"T002\", \"C002\", \"Bob Lee\", \"P002\", \"Jeans\", \"Clothing\", 60.0, 2, datetime(2024, 6, 2, 14, 15), \"Cash\", \"Los Angeles\", False, 0.0, 12, False, \"SR02\"),\n",
    "    (\"T003\", \"C003\", \"Carla Gomez\", \"P003\", \"Organic Apples\", \"Grocery\", 15.0, 5, datetime(2024, 6, 3, 9, 45), \"PayPal\", \"Chicago\", True, 2.0, 3, False, \"SR03\"),\n",
    "    (\"T004\", \"C004\", \"David Kim\", \"P004\", \"Bluetooth Speaker\", \"Electronics\", 85.0, 1, datetime(2024, 6, 4, 16, 0), \"Gift Card\", \"Houston\", False, 0.0, 8, True, \"SR01\"),\n",
    "    (\"T005\", \"C005\", \"Eva Brown\", \"P005\", \"Dress\", \"Clothing\", 120.0, 1, datetime(2024, 6, 5, 11, 20), \"Credit Card\", \"Miami\", True, 20.0, 15, False, \"SR02\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9d8de7-eb4b-4d05-97da-4616eece0a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate additional 1000 rows of random data with the same schema and append it to the\n",
    "original DataFrame (keep 4 partitions) use native python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceabaa66-4dde-459d-842c-beb8dd360c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "first_names = [\"Alice\", \"Bob\", \"Carol\", \"Dave\", \"Eve\"]\n",
    "last_names = [\"Smith\", \"Johnson\", \"Brown\", \"Lee\", \"Garcia\"]\n",
    "cities = [\"New York\", \"San Francisco\", \"Chicago\", \"Austin\", \"Seattle\"]\n",
    "\n",
    "def random_name():\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "def random_city():\n",
    "    return random.choice(cities)\n",
    "\n",
    "def random_date_within_30_days():\n",
    "    now = datetime.now()\n",
    "    delta = timedelta(days=random.randint(0, 30), seconds=random.randint(0, 86400))\n",
    "    return now - delta\n",
    "\n",
    "categories = [\"Electronics\", \"Clothing\", \"Grocery\", \"Toys\", \"Books\"]\n",
    "products = {\n",
    "    \"Electronics\": [(\"P100\", \"Smartphone\"), (\"P101\", \"Tablet\"), (\"P102\", \"Headphones\")],\n",
    "    \"Clothing\": [(\"P200\", \"T-Shirt\"), (\"P201\", \"Jacket\"), (\"P202\", \"Sneakers\")],\n",
    "    \"Grocery\": [(\"P300\", \"Bananas\"), (\"P301\", \"Milk\"), (\"P302\", \"Bread\")],\n",
    "    \"Toys\": [(\"P400\", \"Lego Set\"), (\"P401\", \"Puzzle\"), (\"P402\", \"Action Figure\")],\n",
    "    \"Books\": [(\"P500\", \"Novel\"), (\"P501\", \"Comics\"), (\"P502\", \"Cookbook\")]\n",
    "}\n",
    "payment_modes = [\"Credit Card\", \"Cash\", \"PayPal\", \"Gift Card\", \"Debit Card\"]\n",
    "sales_reps = [\"SR01\", \"SR02\", \"SR03\", \"SR04\"]\n",
    "\n",
    "new_data = []\n",
    "for i in range(1000):\n",
    "    category = random.choice(categories)\n",
    "    product_id, product_name = random.choice(products[category])\n",
    "    amount = round(random.uniform(5, 2000), 2)\n",
    "    quantity = random.randint(1, 5)\n",
    "    discount_applied = random.choice([True, False])\n",
    "    discount_amount = round(random.uniform(0, amount * 0.2), 2) if discount_applied else 0.0\n",
    "    loyalty_points_earned = int(amount // 10)\n",
    "    is_return = random.choice([True, False])\n",
    "    transaction_date = random_date_within_30_days()\n",
    "    row = (\n",
    "        f\"T{1000 + i:03d}\",\n",
    "        f\"C{1000 + i:03d}\",\n",
    "        random_name(),\n",
    "        product_id,\n",
    "        product_name,\n",
    "        category,\n",
    "        amount,\n",
    "        quantity,\n",
    "        transaction_date,\n",
    "        random.choice(payment_modes),\n",
    "        random_city(),\n",
    "        discount_applied,\n",
    "        discount_amount,\n",
    "        loyalty_points_earned,\n",
    "        is_return,\n",
    "        random.choice(sales_reps)\n",
    "    )\n",
    "    new_data.append(row)\n",
    "\n",
    "df_new = spark.createDataFrame(new_data, schema=schema)\n",
    "df_combined = df.union(df_new).repartition(4)\n",
    "df_combined.show(5, truncate=False)\n",
    "print(f\"Total rows: {df_combined.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d50d8a-3050-4d16-8f39-41380108c323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate the code to write to uc and read from it. I am writing and reading to UC instead of creating the parquet and csv files due to missing cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m table = \u001b[33m\"\u001b[39m\u001b[33msales_transactions_uc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Ensure target partitions and write\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df_to_save = \u001b[43mdf_combined\u001b[49m.repartition(\u001b[32m4\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m     spark.sql(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCREATE SCHEMA IF NOT EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatalog\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "# Unity Catalog only: write managed Delta table and validate via SQL (Spark Connect safe)\n",
    "catalog = \"workspace\"   # replace with your UC catalog if different\n",
    "schema = \"default\"     # replace with your UC schema if different\n",
    "table = \"sales_transactions_uc\"\n",
    "\n",
    "# Ensure target partitions and write\n",
    "df_to_save = df_combined.repartition(4)\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "    df_to_save.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table}\")\n",
    "    print(\"Saved Delta table:\", f\"{catalog}.{schema}.{table}\")\n",
    "except Exception as e:\n",
    "    print(\"UC write failed:\", e)\n",
    "\n",
    "try:\n",
    "    # Use SQL to validate (avoids DataFrame.rdd which Spark Connect doesn't implement)\n",
    "    spark.sql(f\"SELECT COUNT(*) AS cnt FROM {catalog}.{schema}.{table}\").show()\n",
    "    spark.sql(f\"DESCRIBE DETAIL {catalog}.{schema}.{table}\").show(truncate=False)\n",
    "    spark.sql(f\"SELECT * FROM {catalog}.{schema}.{table} LIMIT 10\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"UC read/validation failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values and add order_year column\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "# Drop rows with any nulls in any column\n",
    "df_clean = df_combined.dropna()\n",
    "\n",
    "# Add integer order_year extracted from transaction_date\n",
    "df_with_year = df_clean.withColumn(\"order_year\", year(col(\"transaction_date\")))\n",
    "\n",
    "try:\n",
    "    # show basic validation\n",
    "    print(f\"Rows after dropna: {df_with_year.count()}\")\n",
    "    df_with_year.printSchema()\n",
    "    df_with_year.show(10, truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Validation show failed:\", e)\n",
    "\n",
    "# Optionally save cleaned table in UC (uncomment to persist).\n",
    "# Replace catalog/schema/table names if needed.\n",
    "try:\n",
    "    cleaned_table = \"sales_transactions_uc_cleaned\"\n",
    "    df_with_year.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{cleaned_table}\")\n",
    "    print(\"Saved cleaned table:\", f\"{catalog}.{schema}.{cleaned_table}\")\n",
    "except Exception as e:\n",
    "    print(\"Saving cleaned UC table failed (you can ignore if you don't want to persist):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get total sales per year from the table using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total sales per year from UC table (try cleaned table first, fallback to original)\n",
    "catalog = \"workspace\"   # adjust if different\n",
    "schema = \"default\"     # adjust if different\n",
    "cleaned_table = \"sales_transactions_uc_cleaned\"\n",
    "orig_table = \"sales_transactions_uc\"\n",
    "\n",
    "try:\n",
    "    print(f\"Querying cleaned table: {catalog}.{schema}.{cleaned_table}\")\n",
    "    spark.sql(f\"SELECT order_year, SUM(amount * quantity) AS total_sales, COUNT(*) AS transactions FROM {catalog}.{schema}.{cleaned_table} GROUP BY order_year ORDER BY order_year\").show(truncate=False)\n",
    "except Exception as e1:\n",
    "    print('Could not query cleaned table (fallback):', e1)\n",
    "    try:\n",
    "        print(f\"Querying original table and extracting year: {catalog}.{schema}.{orig_table}\")\n",
    "        spark.sql(f\"SELECT YEAR(transaction_date) AS order_year, SUM(amount * quantity) AS total_sales, COUNT(*) AS transactions FROM {catalog}.{schema}.{orig_table} GROUP BY YEAR(transaction_date) ORDER BY YEAR(transaction_date)\").show(truncate=False)\n",
    "    except Exception as e2:\n",
    "        print('Failed to compute total sales per year from UC tables:', e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get total transaction amount per payment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total transaction amount per payment method (try cleaned table first, fallback to original)\n",
    "catalog = \"workspace\"   # adjust if different\n",
    "schema = \"default\"     # adjust if different\n",
    "cleaned_table = \"sales_transactions_uc_cleaned\"\n",
    "orig_table = \"sales_transactions_uc\"\n",
    "\n",
    "try:\n",
    "    print(f\"Querying cleaned table: {catalog}.{schema}.{cleaned_table}\")\n",
    "    spark.sql(f\"SELECT payment_mode, SUM(amount * quantity) AS total_amount, COUNT(*) AS transactions FROM {catalog}.{schema}.{cleaned_table} GROUP BY payment_mode ORDER BY total_amount DESC\").show(truncate=False)\n",
    "except Exception as e1:\n",
    "    print('Could not query cleaned table (fallback):', e1)\n",
    "    try:\n",
    "        print(f\"Querying original table: {catalog}.{schema}.{orig_table}\")\n",
    "        spark.sql(f\"SELECT payment_mode, SUM(amount * quantity) AS total_amount, COUNT(*) AS transactions FROM {catalog}.{schema}.{orig_table} GROUP BY payment_mode ORDER BY total_amount DESC\").show(truncate=False)\n",
    "    except Exception as e2:\n",
    "        print('Failed to compute total transaction amount per payment method from UC tables:', e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize previous queries for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized aggregations (serverless-safe) - NO CACHE TABLE\n",
    "catalog = \"workspace\"   # adjust if different\n",
    "schema = \"default\"     # adjust if different\n",
    "cleaned_table = \"sales_transactions_uc_cleaned\"\n",
    "orig_table = \"sales_transactions_uc\"\n",
    "\n",
    "# Tune shuffle partitions for these aggregations (small number for demo)\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4)\n",
    "\n",
    "try:\n",
    "    # choose source table (cleaned preferred)\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {catalog}.{schema}.{cleaned_table} LIMIT 1\").show()\n",
    "        source = f\"{catalog}.{schema}.{cleaned_table}\"\n",
    "    except Exception:\n",
    "        source = f\"{catalog}.{schema}.{orig_table}\"\n",
    "\n",
    "    # create narrow temp view with only required columns; compute order_year if needed\n",
    "    if source.endswith(cleaned_table):\n",
    "        create_sql = f\"CREATE OR REPLACE TEMP VIEW tmp_sales AS SELECT order_year, amount, quantity, payment_mode FROM {source}\"\n",
    "    else:\n",
    "        create_sql = f\"CREATE OR REPLACE TEMP VIEW tmp_sales AS SELECT YEAR(transaction_date) AS order_year, amount, quantity, payment_mode FROM {source}\"\n",
    "    spark.sql(create_sql)\n",
    "\n",
    "    # 1) total sales per year (amount * quantity) - run on temp view (no cache)\n",
    "    spark.sql(\"\"\"\n",
    "    SELECT order_year,\n",
    "           SUM(amount * quantity) AS total_sales,\n",
    "           COUNT(*) AS transactions\n",
    "    FROM tmp_sales\n",
    "    GROUP BY order_year\n",
    "    ORDER BY order_year\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    # 2) total transaction amount per payment method\n",
    "    spark.sql(\"\"\"\n",
    "    SELECT payment_mode,\n",
    "           SUM(amount * quantity) AS total_amount,\n",
    "           COUNT(*) AS transactions\n",
    "    FROM tmp_sales\n",
    "    GROUP BY payment_mode\n",
    "    ORDER BY total_amount DESC\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    # cleanup temp view\n",
    "    spark.sql(\"DROP VIEW IF EXISTS tmp_sales\")\n",
    "except Exception as e:\n",
    "    print('Optimized aggregation failed (serverless-safe):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify transactions with negative amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify transactions with negative total amount (amount * quantity < 0)\n",
    "catalog = \"workspace\"   # adjust if different\n",
    "schema = \"default\"     # adjust if different\n",
    "cleaned_table = f\"{catalog}.{schema}.sales_transactions_uc_cleaned\"\n",
    "orig_table = f\"{catalog}.{schema}.sales_transactions_uc\"\n",
    "\n",
    "sql_template = \"SELECT transaction_id, customer_id, transaction_date, amount, quantity, (amount * quantity) AS total_amount, payment_mode, store_location FROM {table} WHERE (amount * quantity) < 0 ORDER BY transaction_date DESC LIMIT 100\"\n",
    "\n",
    "try:\n",
    "    print(f\"Querying cleaned table: {cleaned_table}\")\n",
    "    spark.sql(sql_template.format(table=cleaned_table)).show(truncate=False)\n",
    "except Exception as e1:\n",
    "    print('Cleaned table query failed or table missing, falling back to original:', e1)\n",
    "    try:\n",
    "        print(f\"Querying original table: {orig_table}\")\n",
    "        spark.sql(sql_template.format(table=orig_table)).show(truncate=False)\n",
    "    except Exception as e2:\n",
    "        print('Both cleaned and original table queries failed:', e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate if Credit transactions are lower than Debit transactions per each customer and not higher\n",
    "than 10% of the total amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate per-customer: credit < debit AND credit <= 10% of total_amount; show violations\n",
    "catalog = \"workspace\"   # adjust if different\n",
    "schema = \"default\"     # adjust if different\n",
    "cleaned_table = f\"{catalog}.{schema}.sales_transactions_uc_cleaned\"\n",
    "orig_table = f\"{catalog}.{schema}.sales_transactions_uc\"\n",
    "\n",
    "try:\n",
    "    # prefer cleaned table if present\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {cleaned_table} LIMIT 1\").show()\n",
    "        source = cleaned_table\n",
    "    except Exception:\n",
    "        source = orig_table\n",
    "\n",
    "    validation_sql = f'''\n",
    "    WITH agg AS (\n",
    "      SELECT customer_id,\n",
    "             SUM(CASE WHEN lower(payment_mode) LIKE '%credit%' THEN amount * quantity ELSE 0 END) AS total_credit,\n",
    "             SUM(CASE WHEN lower(payment_mode) LIKE '%debit%' THEN amount * quantity ELSE 0 END) AS total_debit,\n",
    "             SUM(amount * quantity) AS total_amount\n",
    "      FROM {source}\n",
    "      GROUP BY customer_id\n",
    "    )\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      total_credit,\n",
    "      total_debit,\n",
    "      total_amount,\n",
    "      CASE WHEN total_amount = 0 THEN 0 ELSE total_credit / total_amount END AS credit_pct,\n",
    "      (total_credit < total_debit) AS credit_lt_debit,\n",
    "      (total_credit <= 0.1 * total_amount) AS credit_le_10pct\n",
    "    FROM agg\n",
    "    WHERE NOT (total_credit < total_debit AND total_credit <= 0.1 * total_amount)\n",
    "    ORDER BY credit_pct DESC\n",
    "    LIMIT 100\n",
    "    '''\n",
    "\n",
    "    spark.sql(validation_sql).show(truncate=False)\n",
    "except Exception as e:\n",
    "    print('Customer-level credit/debit validation failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate calendar DataFrame for year 2024\n",
    "from pyspark.sql.functions import sequence, to_date, explode, col, date_format, dayofweek, weekofyear, quarter, expr\n",
    "\n",
    "start = '2024-01-01'\n",
    "end = '2024-12-31'\n",
    "\n",
    "df_calendar = (\n",
    "    spark.createDataFrame([(start, end)], ['start', 'end'])\n",
    "    .select(sequence(to_date(col('start')), to_date(col('end')), expr('interval 1 day')).alias('dates'))\n",
    "    .select(explode(col('dates')).alias('date'))\n",
    "    .withColumn('year', date_format(col('date'), 'yyyy').cast('int'))\n",
    "    .withColumn('month', date_format(col('date'), 'MM').cast('int'))\n",
    "    .withColumn('day', date_format(col('date'), 'dd').cast('int'))\n",
    "    .withColumn('day_of_week', date_format(col('date'), 'E'))\n",
    "    .withColumn('is_weekend', dayofweek(col('date')).isin(1, 7))\n",
    "    .withColumn('week_of_year', weekofyear(col('date')))\n",
    "    .withColumn('month_name', date_format(col('date'), 'MMMM'))\n",
    "    .withColumn('quarter', quarter(col('date')))\n",
    ")\n",
    "\n",
    "df_calendar.printSchema()\n",
    "df_calendar.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-12-02 23:02:15",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
